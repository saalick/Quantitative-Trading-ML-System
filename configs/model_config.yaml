model:
  type: "LSTM"
  input_size: 45
  hidden_size: 128
  num_layers: 2
  dropout: 0.2
  output_size: 1

training:
  batch_size: 32
  epochs: 50
  learning_rate: 0.001
  optimizer: "SGD"
  momentum: 0.9
  weight_decay: 0.0001
  early_stopping_patience: 10

data:
  sequence_length: 60
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15

features:
  scaling: "standard"
  missing_value_strategy: "drop"
